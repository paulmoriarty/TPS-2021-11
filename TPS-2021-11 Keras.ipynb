{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:50.483206Z",
     "iopub.status.busy": "2021-11-12T15:51:50.483206Z",
     "iopub.status.idle": "2021-11-12T15:51:50.498714Z",
     "shell.execute_reply": "2021-11-12T15:51:50.498714Z",
     "shell.execute_reply.started": "2021-11-12T15:51:50.483206Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ![Screen Shot 2021-11-09 at 7.52.31 AM.png](attachment:2f753b90-c10d-42de-8852-46b36e352d35.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:50.498714Z",
     "iopub.status.busy": "2021-11-12T15:51:50.498714Z",
     "iopub.status.idle": "2021-11-12T15:51:53.795468Z",
     "shell.execute_reply": "2021-11-12T15:51:53.795468Z",
     "shell.execute_reply.started": "2021-11-12T15:51:50.498714Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # suppressing tensorflow warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import seaborn as sns\n",
    "import neptune.new as neptune\n",
    "import glob\n",
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "from datetime import datetime\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    "    StratifiedShuffleSplit,\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    OneHotEncoder,\n",
    ")\n",
    "from scikitplot.metrics import plot_roc, plot_precision_recall\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Flatten, InputLayer, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras_tuner as kt\n",
    "\n",
    "import matplotlib.pyplot as plt  # plotting learning curves for DNN\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"medium\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:53.795468Z",
     "iopub.status.busy": "2021-11-12T15:51:53.795468Z",
     "iopub.status.idle": "2021-11-12T15:51:53.811093Z",
     "shell.execute_reply": "2021-11-12T15:51:53.811093Z",
     "shell.execute_reply.started": "2021-11-12T15:51:53.795468Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "running_on = platform.system()\n",
    "if running_on == \"Darwin\":  # I'm a Mac\n",
    "    drive_path = \"/Users/pmm/My Drive/Colab Notebooks/TPS 2021-11\"\n",
    "elif running_on == \"Windows\":  # I'm a PC\n",
    "    drive_path = r\"C:\\Users\\pmm\\My Drive\\Colab Notebooks\\TPS 2021-11\"\n",
    "elif \"google.colab\" in str(get_ipython()):\n",
    "    drive_path = \"/content\"\n",
    "\n",
    "path = drive_path\n",
    "now = lambda: datetime.now().strftime(\"%Y-%m-%dT%Hh%Mm%Ss\")\n",
    "random_state = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:53.811093Z",
     "iopub.status.busy": "2021-11-12T15:51:53.811093Z",
     "iopub.status.idle": "2021-11-12T15:51:54.529768Z",
     "shell.execute_reply": "2021-11-12T15:51:54.529768Z",
     "shell.execute_reply.started": "2021-11-12T15:51:53.811093Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_feather(os.path.join(path, \"train.feather\")).drop(\n",
    "    columns=[\"id\"]\n",
    ")\n",
    "test_data = pd.read_feather(os.path.join(path, \"test.feather\")).drop(\n",
    "    columns=[\"id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:54.529768Z",
     "iopub.status.busy": "2021-11-12T15:51:54.529768Z",
     "iopub.status.idle": "2021-11-12T15:51:56.389550Z",
     "shell.execute_reply": "2021-11-12T15:51:56.389550Z",
     "shell.execute_reply.started": "2021-11-12T15:51:54.529768Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "scaler = RobustScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler = PowerTransformer()\n",
    "col_names = test_data.columns\n",
    "train_target = train_data[\"target\"]\n",
    "scaled_train_data = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            scaler.fit_transform(train_data.drop(columns=[\"target\"])),\n",
    "            columns=col_names,\n",
    "        ),\n",
    "        train_target,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "scaled_test_data = pd.DataFrame(scaler.transform(test_data), columns=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:56.389550Z",
     "iopub.status.busy": "2021-11-12T15:51:56.389550Z",
     "iopub.status.idle": "2021-11-12T15:51:56.405089Z",
     "shell.execute_reply": "2021-11-12T15:51:56.405089Z",
     "shell.execute_reply.started": "2021-11-12T15:51:56.389550Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# norm = Normalizer()\n",
    "# col_names = scaled_test_data.columns\n",
    "# train_target = scaled_train_data[\"target\"]\n",
    "# norm_train_data = pd.concat(\n",
    "#     [\n",
    "#         pd.DataFrame(\n",
    "#             norm.fit_transform(scaled_train_data.drop(columns=[\"target\"])),\n",
    "#             columns=col_names,\n",
    "#         ),\n",
    "#         train_target,\n",
    "#     ],\n",
    "#     axis=1,\n",
    "# )\n",
    "# norm_test_data = pd.DataFrame(norm.transform(scaled_test_data), columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:56.405089Z",
     "iopub.status.busy": "2021-11-12T15:51:56.405089Z",
     "iopub.status.idle": "2021-11-12T15:51:56.498586Z",
     "shell.execute_reply": "2021-11-12T15:51:56.498586Z",
     "shell.execute_reply.started": "2021-11-12T15:51:56.405089Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = scaled_train_data[\"target\"]\n",
    "X = scaled_train_data.drop(columns=[\"target\"])\n",
    "X_test = scaled_test_data\n",
    "# y = norm_train_data[\"target\"]\n",
    "# X = norm_train_data.drop(columns=[\"target\"])\n",
    "# X_test = norm_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Neptune and beyond..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:56.498586Z",
     "iopub.status.busy": "2021-11-12T15:51:56.498586Z",
     "iopub.status.idle": "2021-11-12T15:51:56.514250Z",
     "shell.execute_reply": "2021-11-12T15:51:56.514250Z",
     "shell.execute_reply.started": "2021-11-12T15:51:56.498586Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_api_token(file):\n",
    "    if os.path.isfile(file):\n",
    "        api_file = open(file, \"r\")\n",
    "        api_token = api_file.readline().rstrip()\n",
    "    else:\n",
    "        print(f\"Neptune API token file {file} not found\")\n",
    "        print(f\"Enter API token: \", end=\"\")\n",
    "        api_token = str(input()).rstrip()\n",
    "    \n",
    "    if api_token == \"\": \n",
    "        sys.exit(\"API token cannot be empty, numbnuts!\")\n",
    "\n",
    "    return api_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:56.514250Z",
     "iopub.status.busy": "2021-11-12T15:51:56.514250Z",
     "iopub.status.idle": "2021-11-12T15:51:59.585523Z",
     "shell.execute_reply": "2021-11-12T15:51:59.585523Z",
     "shell.execute_reply.started": "2021-11-12T15:51:56.514250Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/pmoriarty/Keras-TPS-2021-11/e/KER1-23\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "keyfile = os.path.join(home, \".neptune_api\")\n",
    "project = \"pmoriarty/Keras-TPS-2021-11\"\n",
    "api_token = get_api_token(keyfile)\n",
    "\n",
    "run = neptune.init(project=project, api_token=api_token,)\n",
    "\n",
    "params = {\n",
    "    \"epochs\": 99999,\n",
    "    \"batch_size\": 1024,\n",
    "    \"verbose\": 0,\n",
    "    \"dense_1\": 128,\n",
    "    \"dense_2\": 64,\n",
    "    \"dense_3\": 32,\n",
    "    #     \"dense_4\": 16,\n",
    "    \"drop_1\": 0.5,\n",
    "    \"drop_2\": 0.4,\n",
    "#     \"drop_3\": 0.25,\n",
    "    \"activation\": \"swish\",\n",
    "}\n",
    "\n",
    "run[\"hyper-parameters\"] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network architecture \n",
    "Experimented with number of hidden layers and number of units in layers. Increasing hidden layers upto 4 resulted in minor gains in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:59.585523Z",
     "iopub.status.busy": "2021-11-12T15:51:59.585523Z",
     "iopub.status.idle": "2021-11-12T15:51:59.601407Z",
     "shell.execute_reply": "2021-11-12T15:51:59.601407Z",
     "shell.execute_reply.started": "2021-11-12T15:51:59.585523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=20,\n",
    "    verbose=0,\n",
    "    mode=\"min\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=5, mode=\"min\"\n",
    ")\n",
    "\n",
    "neptune_cbk = NeptuneCallback(run=run, base_namespace=\"metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:59.601407Z",
     "iopub.status.busy": "2021-11-12T15:51:59.601407Z",
     "iopub.status.idle": "2021-11-12T15:51:59.617042Z",
     "shell.execute_reply": "2021-11-12T15:51:59.617042Z",
     "shell.execute_reply.started": "2021-11-12T15:51:59.601407Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(name: str):\n",
    "    input_shape = [X_train.shape[1]]\n",
    "    model = keras.Sequential(name=name)\n",
    "\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "\n",
    "    model.add(Dense(params[\"dense_1\"], activation=params[\"activation\"]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(rate=params[\"drop_1\"]))\n",
    "\n",
    "    model.add(Dense(params[\"dense_2\"], activation=params[\"activation\"]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(rate=params[\"drop_2\"]))\n",
    "\n",
    "    model.add(Dense(params[\"dense_3\"], activation=params[\"activation\"]))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(rate=params[\"drop_3\"]))\n",
    "\n",
    "    #     model.add(Dense(params[\"dense_4\"], activation=params[\"activation\"]))\n",
    "    #     model.add(BatchNormalization())\n",
    "    #     model.add(Dropout(params[\"fourth_drop\"]))\n",
    "\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-12T15:51:59.617042Z",
     "iopub.status.busy": "2021-11-12T15:51:59.617042Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 1: WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0026s). Check your callbacks.\n",
      "Max AUC = 0.759391\n",
      "Fold # 2: Max AUC = 0.755943\n",
      "Fold # 3: Max AUC = 0.760105\n",
      "Fold # 4: Max AUC = 0.756662\n",
      "Fold # 5: Max AUC = 0.759871\n",
      "Fold # 6: "
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=20, shuffle=True, random_state=random_state)\n",
    "# cv = StratifiedShuffleSplit(\n",
    "#     n_splits=5, test_size=0.10, random_state=random_state\n",
    "# )\n",
    "\n",
    "n_folds = range(cv.n_splits)\n",
    "scores = {fold: None for fold in n_folds}\n",
    "predictions = []\n",
    "\n",
    "for fold, (idx_train, idx_val) in enumerate(cv.split(X, y)):\n",
    "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    X_val, y_val = X.iloc[idx_val], y.iloc[idx_val]\n",
    "\n",
    "    model = get_model(name=\"Baseline\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"AUC\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Fold #{fold + 1:2d}: \", end=\"\")\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        # batch_size=1024,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        # epochs=99999,\n",
    "        epochs=params[\"epochs\"],\n",
    "        # verbose=0,\n",
    "        verbose=params[\"verbose\"],\n",
    "        callbacks=[early_stopping, reduce_lr, neptune_cbk],\n",
    "    )\n",
    "\n",
    "    scores[fold] = history.history\n",
    "\n",
    "    max_auc = np.max(scores[fold][\"val_auc\"])\n",
    "    print(f\"Max AUC = {max_auc:8.6f}\")\n",
    "\n",
    "    # thresh = 0.755\n",
    "    # if max_auc < thresh:\n",
    "    #     print(f\"Bailing out! AUC is less than {thresh}\")\n",
    "    #     break\n",
    "\n",
    "    prediction = model.predict(X_test).reshape(1, -1)[0]\n",
    "    predictions.append(prediction)\n",
    "\n",
    "overall_auc = [np.max(scores[fold][\"val_auc\"]) for fold in n_folds]\n",
    "print(f\"Training Mean AUC = {np.mean(overall_auc):8.6f}\")\n",
    "run[\"Training Mean AUC\"] = np.mean(overall_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### plot train versus validation loss for each epoch\n",
    "fig, ax = plt.subplots(5, 4, tight_layout=True, figsize=(20, 20))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for fold in n_folds:\n",
    "    df_eval = pd.DataFrame(\n",
    "        {\"train_auc\": scores[fold][\"auc\"], \"val_auc\": scores[fold][\"val_auc\"],}\n",
    "    )\n",
    "\n",
    "    min_train = np.round(np.min(df_eval[\"train_auc\"]), 5)\n",
    "    min_val = np.round(np.min(df_eval[\"val_auc\"]), 5)\n",
    "    delta = np.round(min_val - min_train, 5)\n",
    "\n",
    "    sns.lineplot(\n",
    "        x=df_eval.index,\n",
    "        y=df_eval[\"train_auc\"],\n",
    "        label=\"train_auc\",\n",
    "        ax=ax[fold],\n",
    "    )\n",
    "\n",
    "    sns.lineplot(\n",
    "        x=df_eval.index, y=df_eval[\"val_auc\"], label=\"val_auc\", ax=ax[fold],\n",
    "    )\n",
    "\n",
    "    ax[fold].set_ylabel(\"\")\n",
    "    ax[fold].set_xlabel(\n",
    "        f\"Fold {fold+1}\\nmin_train: {min_train}\\nmin_val: {min_val}\\ndelta: {delta}\",\n",
    "        fontstyle=\"italic\",\n",
    "    )\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(\"my_model\")\n",
    "\n",
    "run[\"my_model/saved_model\"].upload(\"my_model/saved_model.pb\")\n",
    "for name in glob.glob(\"my_model/variables/*\"):\n",
    "    run[name].upload(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = type(model).__name__\n",
    "submission_file = f\"{drive_path}/submission_{model_name}_{now()}.csv\"\n",
    "\n",
    "# y_pred = model.predict(X_test).ravel()\n",
    "submission = pd.read_csv(f\"{path}/sample_submission.csv\")\n",
    "submission[\"target\"] = np.mean(np.column_stack(predictions), axis=1)\n",
    "submission.to_csv(submission_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params = model.get_weights()\n",
    "val_auc = np.mean(overall_auc)\n",
    "print(\"test auc:\")\n",
    "test_auc = float(input())\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "exp_f = os.path.join(drive_path, \"Experiments.csv\")\n",
    "cols = [\"date\", \"model\", \"params\", \"val_auc\", \"test_auc\"]\n",
    "experiment = pd.DataFrame(\n",
    "    [[timestamp, model_name, model_params, val_auc, test_auc]], columns=cols,\n",
    ")\n",
    "\n",
    "if os.path.exists(exp_f):\n",
    "    experiments = pd.read_csv(exp_f)\n",
    "else:\n",
    "    experiments = pd.DataFrame(columns=cols)\n",
    "\n",
    "experiments = pd.concat([experiments, experiment], ignore_index=True)\n",
    "experiments.to_csv(exp_f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--- Batch Size = 1024 / StratifiedKFold (n_splits=5) / StandardScaler**  \n",
    "Overall Mean AUC:  0.757540249824524   - 128-64  \n",
    "Overall Mean AUC:  0.7558159351348877 - Minus final 64 layer  \n",
    "Overall Mean AUC:  0.7574994564056396 - 64 / Norm / 0.5 dropout  \n",
    "Overall Mean AUC:  0.7572387456893921 - 100-100-100 / Norm / 0.5 dropout  \n",
    "Overall Mean AUC:  0.7573859572410584 - 100-75-50 / Norm / 0.5 dropout  \n",
    "Overall Mean AUC:  0.7493987083435059 - 10-10-10-10-8 / Norm / 0.5  \n",
    "Overall Mean AUC:  0.7573502540588379 - 128-128-128-64 / 0.5 / no Norm on final layer  \n",
    "Overall Mean AUC:  0.7572739481925964 - 128-128-64 / 0.5 / no Norm on final layer  \n",
    "Overall Mean AUC:  0.7574271202087403 - 128-96-128 / 0.5 / no Norm on final layer  \n",
    "Overall Mean AUC:  0.7574907898902893 - 128-96-64 / Norm / 0.5  \n",
    "**--- StratifiedShuffleSplit (n_splits=5)---**  \n",
    "Overall Mean AUC:  0.757282507419586  - 128-96-64 / Norm / 0.5  \n",
    "**--- Added Normalizer after Scaler ---**    \n",
    "Overall Mean AUC:  0.7565243363380432 - 128-96-64 / Norm / 0.5  \n",
    "**--- Test Split = 0.33 ---**    \n",
    "Overall Mean AUC:  0.7556142687797547 - 128-96-64 / Norm / 0.5  \n",
    "**--- Test Split = 0.10 ---**   \n",
    "Overall Mean AUC:  0.7565118908882141 - 128-96-64 / Norm / 0.5  \n",
    "**--- StratifiedKFold ---**    \n",
    "Overall Mean AUC:  0.7566200375556946 - 128-64 / 0.5  \n",
    "Overall Mean AUC:  0.7564255118370056 - 128-64 / Norm / 0.5  \n",
    "**--- Removed Normalizer ---**    \n",
    "Overall Mean AUC:  0.7570324659347534 - 128-64  \n",
    "Overall Mean AUC:  0.7570019364356995 - 128-64 / Norm  \n",
    "Overall Mean AUC:  0.7556699037551879 - 128-64 / Norm / 0.5  \n",
    "**--- Batch Size = 2048 ---**    \n",
    "Overall Mean AUC:  0.7566995620727539 - 128-64  \n",
    "**--- Batch Size = 512 ---**   \n",
    "Overall Mean AUC:  0.7569037437438965 - 128-64\n",
    "Overall Mean AUC:  0.7569687366485596 - 64-32  \n",
    "Overall Mean AUC:  0.7567706227302551 - 32-16\n",
    "Overall Mean AUC:  0.7530458807945252 - 32-24-16  \n",
    "**--(n_splits=10) RobustScaler--**  \n",
    "Overall Mean AUC:  0.7581023335456848 - 128-64 / Norm / 0.5   \n",
    "**--MinMaxScaler--**  \n",
    "Overall Mean AUC:  0.7551696360111236 - 128-64 / Norm / 0.5   \n",
    "**--PowerTransformer--**  \n",
    "Overall Mean AUC:  0.7544887363910675 - 128-64 / Norm / 0.5   \n",
    "**--(n_splits=20) RobustScaler--**  \n",
    "Overall Mean AUC:  0.7586091578006744 - 128-64 / Norm / 0.5   \n",
    "Overall Mean AUC:  0.7508559554815293 - 80-64-32-16 / Norm / 0.5  \n",
    "Overall Mean AUC:  0.754138734936714  - 128 / Norm  \n",
    "Training Mean AUC = 0.758493 - 80-64-32 / Norm  \n",
    "Training Mean AUC = 0.758373 - 80-64-32  \n",
    "Training Mean AUC = 0.758657 - 80-64-32 (.4 dropout)  \n",
    "Training Mean AUC = 0.758585 - 80-64-32 (.4 dropout) / Norm  \n",
    "Training Mean AUC = 0.758751 - 128-64-32 / Norm  \n",
    "Training Mean AUC = 0.758751 - 128-64-32 (.4 dropout) / Norm  \n",
    "Training Mean AUC = 0.758215 - 128-64-32 (.3 dropout) / Norm  \n",
    "Training Mean AUC = 0.756906 - 128-64-32 (.6 dropout) / Norm  \n",
    "Training Mean AUC = 0.758297 - 128-64-32 (.55 dropout) / Norm  \n",
    "Training Mean AUC = 0.758664 = 128-64-32 (.5 / .4 dropout) / Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in glob.glob(\"my_model/variables/*\"):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the architecture to a txt file:\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "with open(f\"./{model_name}_arch.txt\", \"w\") as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()\n",
    "\n",
    "# Log it to Neptune:\n",
    "run[f\"io_files/artifacts/{model_name}_arch\"].upload(f\"./{model_name}_arch.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_gpu():\n",
    "    from numba import cuda\n",
    "\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/mlanhenke/tps-11-nn-baseline-keras  \n",
    "https://www.kaggle.com/stiwar1/tps-nov-21-neural-network-baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "neptune": {
   "notebookId": "bc80a2e6-ac1d-4f5f-ba69-c26505fd04a7",
   "projectVersion": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
